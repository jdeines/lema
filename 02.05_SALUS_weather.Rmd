---
title: "Salus Weather Generator"
author: "Jill Deines"
date: "November 6, 2017"
output: 
  html_document:
    toc: yes
---

Goal: Create SALUS weather input (.wdb.xml) using an automated approach that, based on a shapefile region of interest (roi): 

* extracts NLDAS grid cell polygons with centroid coordinates as attribute data
* clips this to the roi
* joins mean elevation to each NLDAS grid cell
* exports NLDAS polygons in the roi for downstream distribution of SALUS results
* extracts NLDAS data from the SALUS-hosted web tool and updates the elevation data, exports to drive
* re-loads weather wdb.xml's to clean -9999 missing values (linear interpolations)

Future goal: turn this into a function in the salustools r package


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path='figure/02.05_SALUS_weather/',
                      cache = TRUE)
```

**R Packages Needed**

```{r packages, warning=FALSE, message=FALSE}
# to get nldas polygons and centroids for region of interest
library(dplyr)
library(rgdal)
library(raster)
library(sf)

# for nldas data downloading
library(lubridate)
library(xml2)
library(stringr)

# for missing values
library(zoo)
```

# User Variables 

### user variables: Getting NLDAS Centroids

```{r userVars1}
# boundary shapefile 
aoiFile <- 'C:/Users/deinesji/Dropbox/1PhdJill/hpa/LEMAs/data/GIS/boundaries/GMD4_plus.shp'

# nldas elevation data
elevationDataFile <- 'C:/Users/deinesji/Dropbox/1PhdJill/hpa/LEMAs/data/salus/weather/nldas_Jill/nldas_elevationLookup/NLDAS_meanElevationDataset_meters.txt'

# output folder for clipped polygons and csv coordinates
outDir_nldasSubset <- 'C:/Users/deinesji/Dropbox/1PhdJill/hpa/LEMAs/data/salus/weather/nldas_Jill/nldas_centroidExtract'
outName_nldasPoly <- 'NLDAS_clipped_gmd4p.shp'
outName_nldasText <- 'NLDAS_clipped_gmd4p_centroids_elev.csv'
```

### User Variables: Downloading NLDAS data 

```{r userVars2}
# centroids file (default: use one created in first half of script)
centroidsFile <- paste0(outDir_nldasSubset,'/',outName_nldasText)

# NLDAS date bounds
startDate <- '2000-01-01'
endDate <- '2017-10-31'

# output directories
outXmlDir <- 'C:/Users/deinesji/Dropbox/1PhdJill/hpa/LEMAs/data/salus/weather/nldas_Jill/nldas_wdb_xmls_gmd4p'
```

### User Variables: Cleaning NLDAS data

```{r userVars2}
# sml dir (default: use one created in script)
xmlDir <- outXmlDir

# NLDAS date bounds
startDate2 <- startDate
endDate2 <- endDate

# maximum number of missing values to fill
maxFill <- 3

# output directory 
outCleanDir <- paste0(outXmlDir,'_cleaned')
```

**END USER dEFINED VARIABLES**

# Extract NLDAS Cell Centroids in Study Area

## Create the NLDAS grid
Raster parameters for NDLAS. gridID's assigned as cell numbers starting at upper left corner.

```{r getGrid}
# create a NLDAS raster grid template (full US) using grid specs
adjust <- 0.0625 # half of the 1/8 degree grid
nldasTemplate <- raster(nrow = 224, ncol = 464, crs = "+proj=longlat +datum=WGS84",
                     extent(-124.9375 - adjust,     #ymin
                            -67.0625 + adjust,      #ymax
                            25.0625 - adjust,       #xmin
                            52.9375 + adjust))      #xmax
# # assign grid ID's
# nldasTemplate[] <- 1:ncell(nldasTemplate)
```

## Clip NLDAS to Study Area
Clip grid by a buffered boundary to ensure covering cell centroids, polygonize, calculate centroid coordinates, and intersect to get cells in boundary.

Assigns grid cell IDs based on the buffered, cropped NLDAS subset.

```{r nldasOut, warning=FALSE}
# load boundary and project to a meters based projection
boundary <- read_sf(aoiFile) %>% st_transform(4326)

# buffer boundary to include cells whose centroid is outside boundary
buffed <- boundary %>% st_transform(5070) %>% 
  st_buffer(10000) %>% st_transform(4326)

# convert class to interface with rasters
buffed.spdf <- as(buffed, "Spatial")

# crop nldas grid to new boundary and polygonize
nldasCrop <- crop(nldasTemplate, buffed.spdf)

# number nldas cells for study area?
nldasCrop[] <- 1000:(1000+ncell(nldasCrop))

# polygonize
nldas.poly <- rasterToPolygons(nldasCrop)

# rename grid cell column
names(nldas.poly) <- 'gridCell'

# add centroids to interface with elevation table and data retriver
centroids <- st_as_sf(nldas.poly) %>% 
  st_centroid() 
pts <- do.call(rbind, st_geometry(centroids))

nldas.poly$CENTROID_X <- pts[,1]
nldas.poly$CENTROID_Y <- pts[,2]

# clip nldas polygons to precise study boundary
nldasClipped <- st_as_sf(nldas.poly) %>% st_intersection(boundary)

# vis
plot(nldasClipped[1], key.pos=1)
plot(boundary[1], col='transparent', add=TRUE, lty = 2, lwd = 2)
```


## Add NLDAS elevation data 
SALUS needs weather station elevation, so I'm going to use NLDAS cell elevations.

This mean elevation dataset was downloaded from the LDAS website at https://ldas.gsfc.nasa.gov/nldas/NLDASelevation.php

Per the metadata there, it's a 1/8th degree resolution topographic dataset based on GTOPO30 Global 30 Arc Second (~1 km) Elevation Dataset

```{r addElev}
# load elevation table
elevs <- read.table(elevationDataFile)
names(elevs) <- c('Col','Row','Lat','Lon','meanElev')

# make a lat/long column for matching
elevs$coordsString <- paste0(elevs$Lat,elevs$Lon)
nldasClipped$coordsString <- paste0(nldasClipped$CENTROID_Y,nldasClipped$CENTROID_X)

# combine
nldasClipped2 <- nldasClipped %>% left_join(elevs) 

plot(nldasClipped2['meanElev'], key.pos=1, main = 'Mean Elevation (m)')
```

## Export
Export both a polygon key for the NLDAS grid with locally numbered cells, and a csv file that can be fed into the NLDAS retrieval routine.

```{r export}
# a shapefile of the clipped NLDAS grid polygons
write_sf(nldasClipped2, paste0(outDir_nldasSubset,'/',outName_nldasPoly))

# format for text export
nldas.df <- as.data.frame(nldasClipped2) %>%
  dplyr::select(gridCell,CENTROID_X,CENTROID_Y,meanElev)
write.csv(nldas.df, paste0(outDir_nldasSubset,'/',outName_nldasText), row.names=FALSE)
```

# Download NLDAS data for centroids
This basically mimics Lydia Rill's `extract_NLDAS.py` script with the exception of automatically replacing elevation with the NLDAS cell mean elevations

Overview:

* loads NLDAS coordinates CSV generated in previous section
* specifies the time frame of data to download
* generates a request URL for [SALUS's online NLDAS interface](http://salusmodel.glg.msu.edu/NLDAS/) 
* downloads and formats xml
* updates the elevation number to the NLDAS mean (since it's possible the Google tool hits its max daily quota) 
* writes out the NLDAS time series for each NLDAS cell coordinate


```{r loadCentroids, eval=FALSE}
# load centroids from above
centroids <- read.csv(centroidsFile)

# salus nldas online tool 
baseurl <- 'http://salusmodel.glg.msu.edu/NLDAS/NLDAS2.php?'

# loop through centroid rows to get, clean, export .wdb.xml's
for (i in 1:nrow(centroids)){
  # make query url
  GETrequest <- paste0(baseurl,
                       'lat=',    round(centroids$CENTROID_Y[i],4),
                       '&lon=',   round(centroids$CENTROID_X[i],4),
                       '&smon=',  sprintf("%02d",month(startDate)),
                       '&sdom=',  sprintf("%02d",yday(startDate)),
                       '&syear=', year(startDate),
                       '&emon=',  sprintf("%02d",month(endDate)),
                       '&edom=',  sprintf("%02d",day(endDate)),
                       '&eyear=', year(endDate),
                       '&si=',    centroids$gridCell[i],
                       '&sn=',    centroids$gridCell[i])
  
  # retrive data
  request <- read_xml(GETrequest)
  text <- xml_text(request)
  char_first <- str_locate(text, '<WDB>')[1] 
  char_last <- str_locate(text, '</WDB>')[2]
  textClean <- str_sub(text, char_first, char_last)
  
  # replace elevation data
  nldasElev <- round(centroids$meanElev[i],2)
  textElev <- str_replace(textClean,
                          'Elev="([\\d]+).([\\d]+)"',
                          paste0('Elev="',nldasElev,'"'))
  
  # write to file
  fileConn<-file(paste0(outXmlDir,"/",centroids$gridCell[i],".wdb.xml"))
  writeLines(textElev, fileConn)
  close(fileConn)
}
```


# Clean NLDAS data
Occasionally NLDAS has missing -9999 values. Current fill method: interpolate missing values.

I currently check for any values < -98, since I'm not sure about the consistency of the no data flag, and it's incredibly unlikely any actual data values are that low.

```{r fillMissing}
# get files
files.long <- list.files(xmlDir, full.names=TRUE)
files.short <- list.files(xmlDir)
# number of front matter lines to skip 
skipLines <- 7

# process each in a loop
for(m in 1:length(files.long)){
  
  # count the number of days (aka, data rows)
  daynum <- length(seq(as.Date(startDate2), as.Date(endDate2), by = 'days'))
  cols <- c('Year','DOY','SRAD','Tmax','Tmin','Rain','DewP','Wind','PAR')
  
  # load the df part of the xmls
  wdata <- read.table(files.long[m], sep = ',', col.names=cols,
                      skip = skipLines, nrows=daynum)
  # convert missing data flags to NA (anything -99 or less)
  wdata[wdata < -98] <- NA
  
  # test for missing data, and update values if needed
  if (sum(is.na(wdata$SRAD)) > 0 | sum(is.na(wdata$Tmax)) > 0 |
      sum(is.na(wdata$Tmin)) > 0 | sum(is.na(wdata$Rain)) > 0) {
    print('missing data, filling in')
    
    # fill in the values via linear interpolation
    wdata <- wdata %>%
      mutate(SRAD = zoo::na.approx(SRAD, maxgap = maxFill),
             Tmax = zoo::na.approx(Tmax, maxgap = maxFill),
             Tmin = zoo::na.approx(Tmin, maxgap = maxFill),
             Rain = zoo::na.approx(Rain, maxgap = maxFill))
    
    # check for remaining NA's
    if (sum(is.na(wdata$SRAD)) > 0 | sum(is.na(wdata$Tmax)) > 0 |
    sum(is.na(wdata$Tmin)) > 0 | sum(is.na(wdata$Rain)) > 0) {
      print(paste(files.short[m],'still missing data'))
    }
  }
  
  # grab original header/footer xml material
  topMatter <- readLines(files.long[m], n=7)
  allMatter <- readLines(files.long[m])
  bottomMatter <- allMatter[(skipLines+daynum+1):length(allMatter)]
  
  # write to file
  fileConn<-file(paste0(outCleanDir,"/",files.short[m]))
  writeLines(topMatter, fileConn)
  close(fileConn)
  write.table(wdata, paste0(outCleanDir,"/",files.short[m]), append=TRUE, 
              sep = ',', row.names=FALSE, col.names=FALSE)
  cat(bottomMatter,file=paste0(outCleanDir,"/",files.short[m]), append=TRUE, sep='\n')

  
  fileConn<-file(paste0(outCleanDir,"/",files.short[m]))
  writeLines(bottomMatter, fileConn)
  close(fileConn)
}


```

